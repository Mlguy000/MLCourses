{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './data/')\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy LSTM cell, 1 layer, 5 hidden units (h and c of size (1,5)\n",
    "LSTMCellSize = 5\n",
    "LSTMCell = tf.keras.layers.LSTMCell(LSTMCellSize)\n",
    "h = tf.zeros(shape=[1,LSTMCellSize], dtype=tf.float32)\n",
    "c = tf.ones(shape=[1,LSTMCellSize], dtype=tf.float32)\n",
    "state = (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tf.constant([[1,2,3,4,5,6,7]],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, newState = LSTMCell(sample_input, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(newState))\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy LSTM cell, 2 layers, 7 hidden units + 4 hidden units\n",
    "l1 = 7\n",
    "l2 = 4\n",
    "cells = []\n",
    "\n",
    "cell1 = tf.contrib.rnn.LSTMCell(l1)\n",
    "cells.append(cell1)\n",
    "h1 = tf.zeros(shape=[1,l1], dtype=tf.float32)\n",
    "c1 = tf.ones(shape=[1,l1], dtype=tf.float32)\n",
    "state1 = (h1,c1)\n",
    "\n",
    "cell2 = tf.contrib.rnn.LSTMCell(l2)\n",
    "cells.append(cell2)\n",
    "h2 = tf.zeros(shape=[1,l2], dtype=tf.float32)\n",
    "c2 = tf.ones(shape=[1,l2], dtype=tf.float32)\n",
    "state2 = (h1,c1)\n",
    "\n",
    "stackedLSTM = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "input_dimension = 10\n",
    "data = tf.placeholder(tf.float32, shape=[None, None, input_dimension])\n",
    "\n",
    "output, newState = tf.nn.dynamic_rnn(stackedLSTM, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatches = 3 \n",
    "nSteps = 2\n",
    "sample_input = np.array([[[i+10*j+100*k for i in range(input_dimension)] for j in range(nBatches)] for k in range(nSteps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #print(sess.run(newState, feed_dict={data: sample_input}))\n",
    "    #print(sess.run(output, feed_dict={data: sample_input}))\n",
    "    print(\"Input shape: \", sample_input.shape)\n",
    "    print(\"First cell: \", sess.run(newState, feed_dict={data: sample_input})[0][1].shape)\n",
    "    print(\"Second cell: \", sess.run(newState, feed_dict={data: sample_input})[1][1].shape)\n",
    "    print(\"Output shape: \", sess.run(output, feed_dict={data: sample_input}).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LANGUAGE MODELLING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting parameters:\n",
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 30\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size = 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{batch:30x}(steps:20x)200 input units \n",
    "-> {batch:30x}[256] Weight -> 256 Hidden units (first layer) \n",
    "-> {batch:30x}[128] Weight matrix  -> 128 Hidden_units (second layer) ->  \n",
    "Softmax layer -> {batch:30x}(steps:20x)200 unit output\n",
    "\n",
    "$$Softmax = [600(batch:30 \\times steps:20) \\times 128] * [128 \\times 10000] + [1 \\times 10000] \\Longrightarrow [600 \\times 10000]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./data\\reader.py:30: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "Total number of train words:  929589\n",
      "First 10 word ids  [9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983]\n",
      "Corresponding words  ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec']\n"
     ]
    }
   ],
   "source": [
    "#One step manually\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data\n",
    "\n",
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "print(\"Total number of train words: \", len(train_data))\n",
    "print(\"First 10 word ids \", train_data[0:10])\n",
    "print(\"Corresponding words \", id_to_word(train_data[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  (30, 20)\n",
      "First 3 sentences (ids):  [[9970 9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988\n",
      "  9989 9991 9992 9993 9994 9995]\n",
      " [2654    6  334 2886    4    1  233  711  834   11  130  123    7  514\n",
      "     2   63   10  514    8  605]\n",
      " [   0 1071    4    0  185   24  368   20   31 3109  954   12    3   21\n",
      "     2 2915    2   12    3   21]]\n",
      "First sentence:  ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim']\n",
      "Targets for 1st sentence:  ['banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food']\n"
     ]
    }
   ],
   "source": [
    "#We will read 1 mini-batch\n",
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]\n",
    "\n",
    "print(\"Batch size: \", x.shape)\n",
    "print(\"First 3 sentences (ids): \", x[0:3])\n",
    "print(\"First sentence: \", id_to_word(x[0]))\n",
    "print(\"Targets for 1st sentence: \", id_to_word(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last sentence in the current batch:\n",
      "['also', 'returned', 'contributions', 'he', 'received', 'from', 'mr.', 'keating', 'a', 'year', 'ago', '<eos>', 'sens.', 'john', 'glenn', 'd.', 'ohio', 'john', '<unk>', 'r.']\n"
     ]
    }
   ],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20] 1 batch of (30) 20-word sentences\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "feed_dict = {_input_data:x, _targets:y}\n",
    "print(\"Last sentence in the current batch:\")\n",
    "print(id_to_word(session.run(_input_data, feed_dict)[30-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>_initial_state</h4>\n",
    "\n",
    "For each LCTM, there are 2 state matrices, c\\_state and m\\_state.  c_state and m_state represent \"Memory State\" and \"Cell State\". \n",
    "Each hidden layer, has a vector of size 30, which keeps the states. so, for 256/128 hidden units in each LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-f5971052011e>:4: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-8-f5971052011e>:7: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros:0' shape=(30, 256) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros_1:0' shape=(30, 256) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros:0' shape=(30, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros_1:0' shape=(30, 128) dtype=float32>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Two LSTM cells\n",
    "cells = []\n",
    "\n",
    "cell1 = tf.contrib.rnn.LSTMCell(hidden_size_l1)\n",
    "cell2 = tf.contrib.rnn.LSTMCell(hidden_size_l2)\n",
    "\n",
    "stackedLSTMCells = tf.contrib.rnn.MultiRNNCell((cell1, cell2))\n",
    "\n",
    "_initial_state = stackedLSTMCells.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Embeddings</h3>\n",
    "We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values. However, One-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient. So, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM. \n",
    "\n",
    "The embedded vectors also get updated during the training process of the deep neural network.\n",
    "We create the embeddings for our input data. <b>embedding_vocab</b> is matrix of [10000x200] for all 10000 unique words.\n",
    "\n",
    "<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>\n",
    "It creates a [30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])  #[10000x200]\n",
    "inputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  #_input_data(30,20) => inputs(30, 20, 200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Constructing Recurrent Neural Networks</h3>\n",
    "<b>tf.nn.dynamic_rnn()</b> creates a recurrent neural network using <b>stacked_lstm</b>. \n",
    "\n",
    "The input should be a Tensor of shape: [batch_size, max_time, embedding_vector_size], in our case it would be (30, 20, 200)\n",
    "\n",
    "This method, returns a pair (outputs, new_state) where:\n",
    "<ul>\n",
    "    <li><b>outputs</b>: is a length T list of outputs (one for each input), or a nested tuple of such elements.</li>\n",
    "    <li><b>new_state</b>: is the final state.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-a641dbaba858>:1: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stackedLSTMCells, inputs, initial_state=_initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_input_data: 1 num_steps-word Batch, Before embedding\n",
      "(30, 20)\n",
      "inputs: 1 um_steps-word Batch, After embedding\n",
      "(30, 20, 200)\n",
      "_initial_state: First cell\n",
      "(30, 256)\n",
      "_initial_state: Second cell\n",
      "(30, 128)\n",
      "Going through the stacked cells... One 200-embedded word per time, 20 times, for all 30 batches\n",
      "PER BATCH: Each of the 20 (200-embedded) word will go through the 256/128-node cell\n",
      "new_state: First cell\n",
      "(30, 256)\n",
      "new_state: Second cell\n",
      "(30, 128)\n",
      "outputs\n",
      "(30, 20, 128)\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"_input_data: 1 num_steps-word Batch, Before embedding\")\n",
    "print(session.run(_input_data, feed_dict).shape)\n",
    "print(\"inputs: 1 um_steps-word Batch, After embedding\")\n",
    "print(session.run(inputs, feed_dict).shape)\n",
    "\n",
    "print(\"_initial_state: First cell\")\n",
    "print(session.run(_initial_state, feed_dict)[0][1].shape)\n",
    "print(\"_initial_state: Second cell\")\n",
    "print(session.run(_initial_state, feed_dict)[1][1].shape)\n",
    "\n",
    "print(\"Going through the stacked cells... One 200-embedded word per time, 20 times, for all 30 batches\")\n",
    "print(\"PER BATCH: Each of the 20 (200-embedded) word will go through the 256/128-node cell\")\n",
    "\n",
    "print(\"new_state: First cell\")\n",
    "print(session.run(new_state, feed_dict)[0][1].shape)\n",
    "print(\"new_state: Second cell\")\n",
    "print(session.run(new_state, feed_dict)[1][1].shape)\n",
    "\n",
    "print(\"outputs\")\n",
    "print(session.run(outputs, feed_dict).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to flatten the outputs to be able to connect it softmax layer. Lets reshape the output tensor from  [30 x 20 x 128] to [600 x 128]\n",
    "\n",
    "<b>Notice:</b> Imagine our output is 3-d tensor as following (of course each <code>sen_x_word_y</code> is a an embedded vector by itself): \n",
    "<ul>\n",
    "    <li>sentence 1: [[sen1word1], [sen1word2], [sen1word3], ..., [sen1word20]]</li> \n",
    "    <li>sentence 2: [[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20]]</li>   \n",
    "    <li>sentence 3: [[sen3word1], [sen3word2], [sen3word3], ..., [sen3word20]]</li>  \n",
    "    <li>...  </li>\n",
    "    <li>sentence 30: [[sen30word1], [sen30word2], [sen30word3], ..., [sen30word20]]</li>   \n",
    "</ul>\n",
    "Now, the flattened would convert this 3-dim tensor to:\n",
    "\n",
    "[ [sen1word1], [sen1word2], [sen1word3], ..., [sen1word20],[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20], ..., [sen30word20] ]\n",
    "\n",
    "<h3>logistic unit</h3>\n",
    "Now, we create a logistic unit to return the probability of the output word in our vocabulary with 1000 words. \n",
    "$$Softmax = [600(batch:30 \\times steps:20) \\times 128] * [128 \\times 10000] + [1 \\times 10000] \\Longrightarrow [600 \\times 10000]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable softmax_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-e15b5476a56a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size_l2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoftmax_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax_w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mhidden_size_l2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#[128x1000]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msoftmax_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax_b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#[1x10000]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftmax_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msoftmax_b\u001b[0m \u001b[1;31m#600x10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1243\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"tensorflow/python\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[1;32m--> 868\u001b[1;33m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable softmax_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Nidhal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[128x1000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x10000]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b #600x10000\n",
    "prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_input_data: 1 num_steps-word Batch, Before embedding\n",
      "(30, 20)\n",
      "inputs: 1 um_steps-word Batch, After embedding\n",
      "(30, 20, 200)\n",
      "_initial_state: First cell\n",
      "(30, 256)\n",
      "_initial_state: Second cell\n",
      "(30, 128)\n",
      "Going through the stacked cells... One 200-embedded word per time, 20 times, for all 30 batches\n",
      "PER BATCH: Each of the 20 (200-embedded) word will go through the 256/128-node cell\n",
      "new_state: First cell\n",
      "(30, 256)\n",
      "new_state: Second cell\n",
      "(30, 128)\n",
      "outputs\n",
      "(30, 20, 128)\n",
      "output (without S!)\n",
      "(600, 128)\n",
      "softmax_w\n",
      "(128, 10000)\n",
      "softmax_b\n",
      "(10000,)\n",
      "logits: output*sotfmaw_w + sotfmax_b\n",
      "(600, 10000)\n",
      "prob: Softmax(logits)\n",
      "(600, 10000)\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"_input_data: 1 num_steps-word Batch, Before embedding\")\n",
    "print(session.run(_input_data, feed_dict).shape)\n",
    "print(\"inputs: 1 um_steps-word Batch, After embedding\")\n",
    "print(session.run(inputs, feed_dict).shape)\n",
    "\n",
    "print(\"_initial_state: First cell\")\n",
    "print(session.run(_initial_state, feed_dict)[0][1].shape)\n",
    "print(\"_initial_state: Second cell\")\n",
    "print(session.run(_initial_state, feed_dict)[1][1].shape)\n",
    "\n",
    "print(\"Going through the stacked cells... One 200-embedded word per time, 20 times, for all 30 batches\")\n",
    "print(\"PER BATCH: Each of the 20 (200-embedded) word will go through the 256/128-node cell\")\n",
    "\n",
    "print(\"new_state: First cell\")\n",
    "print(session.run(new_state, feed_dict)[0][1].shape)\n",
    "print(\"new_state: Second cell\")\n",
    "print(session.run(new_state, feed_dict)[1][1].shape)\n",
    "\n",
    "print(\"outputs\")\n",
    "print(session.run(outputs, feed_dict).shape)\n",
    "print(\"output (without S!)\")\n",
    "print(session.run(output, feed_dict).shape)\n",
    "\n",
    "print(\"softmax_w\")\n",
    "print(session.run(softmax_w, feed_dict).shape)\n",
    "print(\"softmax_b\")\n",
    "print(session.run(softmax_b, feed_dict).shape)\n",
    "print(\"logits: output*sotfmaw_w + sotfmax_b\")\n",
    "print(session.run(logits, feed_dict).shape)\n",
    "print(\"prob: Softmax(logits)\")\n",
    "print(session.run(prob, feed_dict).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prediction</h3>\n",
    "What is the word correspond to the probability output? Lets use the maximum probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\n",
      "['million', 'handicapped', 'handicapped', 'handicapped', 'handicapped', 'handicapped', 'dreyfus', 'dreyfus', 'approved', 'approved', 'approved', 'settlement', 'settlement', 'settlement', 'settlement', 'settlement', 'fly', 'roles', 'rivals', 'fly']\n",
      "Truth\n",
      "['banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food']\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted\")\n",
    "print(id_to_word(np.argmax(session.run(prob, feed_dict)[0:20], axis=1)))\n",
    "print(\"Truth\")\n",
    "print(id_to_word(session.run(_targets, feed_dict)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Objective function</h4>\n",
    "\n",
    "Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n",
    "\n",
    "This function is already implemented and available in TensorFlow through <b>sequence_loss_by_example</b>. It calculates the weighted cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.  \n",
    "\n",
    "The arguments of this function are:  \n",
    "<ul>\n",
    "    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  \n",
    "    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   \n",
    "    <li>weights: List of 1D batch-sized float-Tensors of the same length as logits.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])\n",
    "cost = tf.reduce_sum(loss) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 log-perplexities\n",
      "[9.227823 9.206076 9.21901  9.198953 9.217763 9.219764 9.196862 9.213927\n",
      " 9.202095 9.208251]\n",
      "Cost\n",
      "184.25726\n"
     ]
    }
   ],
   "source": [
    "print(\"The first 10 log-perplexities\")\n",
    "print(session.run(loss, feed_dict)[:10])\n",
    "print(\"Cost\")\n",
    "print(session.run(cost, feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>\n",
    "\n",
    "To do training for our network, we have to take the following steps:\n",
    "<ol>\n",
    "    <li>Define the optimizer.</li>\n",
    "    <li>Extract variables that are trainable.</li>\n",
    "    <li>Calculate the gradients based on the loss function.</li>\n",
    "    <li>Apply the optimizer to the variables/gradients tuple.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Optimizer\n",
    "lr = tf.Variable(0.1, trainable=False)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "#Extract variables that are trainable\n",
    "tvars = tf.trainable_variables()\n",
    "#Calculate the gradients based on the loss function\n",
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "#Clip norms\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "#Apply the optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 log-perplexities\n",
      "[9.216266  9.210908  9.209421  9.208688  9.191307  9.193218  9.19128\n",
      " 9.219045  9.2014475 9.191616 ]\n",
      "Cost\n",
      "183.68549\n"
     ]
    }
   ],
   "source": [
    "print(\"The first 10 log-perplexities\")\n",
    "print(session.run(loss, feed_dict)[:10])\n",
    "print(\"Cost\")\n",
    "print(session.run(cost, feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
